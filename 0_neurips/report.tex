\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
\PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2023


% ready for submission
% \usepackage{neurips_2023}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
\usepackage[preprint]{neurips_2023}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2023}


% to avoid loading the natbib package, add option nonatbib:
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{graphicx}


\title{Reinforcement Learning for Tower Defence}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{
  Andrew Wallace\\
  101210291\\
  \texttt{andrewwallace3@cmail.carleton.ca}
  \AND
  Derrick Zhang\\
  101232374\\
  \texttt{derrickzhang@cmail.carleton.ca}
  \AND
  Mohammad Rehman\\
  101220514\\
  \texttt{mohammadrehman@cmail.carleton.ca}
  \AND
  Manal Hassan\\
  101263813\\
  \texttt{manalhassa@cmail.carleton.ca}
}


\begin{document}


\maketitle

\author{Derrick Zhang - 101232374}


\begin{abstract}
  The abstract paragraph should be indented \nicefrac{1}{2}~inch (3~picas) on
  both the left- and right-hand margins. Use 10~point type, with a vertical
  spacing (leading) of 11~points.  The word \textbf{Abstract} must be centered,
  bold, and in point size 12. Two line spaces precede the abstract. The abstract
  must be limited to one paragraph.
\end{abstract}


\section{Introduction}

\subsection{Project focus}

Tower defence is a strategy game where the goal is to place defensive structures to obstruct and defeat waves of enemies from reaching a goal. This is done by placing and upgrading towers along the attacker's path. The towers are stationary objects that the agent places to attack enemies. These towers have a cost to create and upgrade, meaning the agent must gather resources to do so. In traditional tower defence games, the player starts with a set number of resources and can obtain more after successfully defending from a wave or defeating enemies. The defensive tower upgrades can vary in range and damage.

The objective of this project is to train an agent using reinforcement learning (RL) methods to learn an optimal policy for tower placement that maximizes survival and minimizes damage taken from enemies. Unlike traditional tower defence games with manual upgrades, our environment features towers that automatically evolve and strengthen based on the number of enemies they eliminate, creating a feedback loop between decisions and long-term strategy when it comes to positioning.

\subsection{Importance of the problem}

Exploring RL for tower defence is extremely valuable. Applying RL methods in high-dimensional state and action spaces, challenging reward structures, and its application in balancing short and long-term investment, giving us valuable insights into the capabilities of RL in highly transferrable domains.

\subsection{MDP specifications}
The state space is represented as a 3d box the shape of [ 10, 10, [8] ], 10 and 10 representing the x, y coordinate of a cell on the grid and the third dimension being an array containing:\\\\Tower ID , Tower Level , Number Of Enemies, Average Health Of Enemies, Enemy With The Highest Health Value, If The Cell Has An Enemy, If The Cell Is The Base, and The health of the base.\\\\The state space would be the addition of all the max possible values of the third dimension to the power of cell on the grid:\\\\
\[(3+6+11+51+51+2+2+41)^{100} = 167^{100}\] \\\\ The action space is of size 97, the agent can either do nothing or place one of two types of turrets on the 49 viable positions on the grid. The agent has the choice of placing down a single target tower, an area of effect tower or do nothing. In this project the agent acts in phases, there’s a build phase and a defense phase. In the build phase the agent is allowed to place one of two types of towers or be inactive. While in the defense phase, the agent is allowed to place towers so long as he has the budget to, but the waves of enemies will begin spawning.\\\\The reward structure is the following:\\\\
Enemy Defeated = +10\\
Enemy Reaches Base  = -15\\
Enemy Damaged = +1\\
Tower Level’s up = +5\\
Wave Cleared = +20\\
All Waves Cleared = +200\\
Base Destroyed = -50\\
Invalid or No Action = -1
\begin{figure}[!htbp]
  \centering
  \includegraphics[width=0.5\linewidth]{figures/mdpspecs_screenshots.png}
  \caption{state action diagram}
  \label{fig:mdp_state_action}
\end{figure}


\section{Approaches}
\subsection{Prior work}

Previous work by Maria Manolaki also focused on implementing reinforcement learning (RL) for tower defence for learning an optimal policy \cite{manolaki_td_rl}. However, in their implementation, the enemy was the artificial intelligence agent. The agent's goal was to learn an optimal policy to win the game. This involves the enemy avoiding towers and finding the best path to the base. They implemented the A2C algorithm to provide the enemy with intelligence needed to reach the base. Our implementation differs, focusing only on the player agent. It would be interesting to build on this by deploying a multi-agent interaction between a player and an enemy agent and  analyze their interaction.

Another application of RL for tower defence was implemented by Timothée Blondiaux et al. for Plants vs. Zombies, a popular tower defence game consisting of plants as towers and zombies as enemies \cite{blondiaux_pvz}. They implemented a wide array of RL methods, including A2C, Monte-Carlo policy gradient, Deep Q-Networks (DQN), and Double Deep Q-Networks (DDQN). They observed good results with the DDQN algorithm, beating difficult levels 65\% of the time \cite{blondiaux_pvz}.

A more general approach was taken in a study by Abhinav Wasukar and Sumitra Jakhete, looking at the application of the DQN and PPO algorithms in various game environments, including tower defence. They found that PPO significantly outperformed DQN, achieving a much higher average return. However, DQN showed lower variance and was more consistent  across runs \cite{wasukar2024_dqn_ppo}. 

One interesting (and indirect) study done by Baylor Wetzel, studied Transfer Learning in a tower defence environment. Transfer learning is the process of using knowledge gained while solving one problem to solve a new, previously unencountered problem \cite{wetzel2011_transfer}. In this study, they collected data from human reported solutions to a tower defence game. They used these solutions to create computational agents to model human strategies. Then, a domain expert was asked to tell apart the human-made tower placements from the computer-generated ones - they could not tell the difference \cite{wetzel2011_transfer}. This approach focuses on modelling human behaviour rather than achieving optimal performance.

A similar study done by Augusto Dias et al., utilized game metadata as inputs to a Deep Q-Learning agent rather than video frames or pixel information \cite{dias2022_td}. They found that no other work reported developing autonomous agents to play tower defence games (2020) and that using metadata as input reduced computational complexity \cite{dias2022_td}. This is similar to our implementation where state is represented by the game metadata.

\subsection{Our approaches}

Our project differs from these works through the automatic tower level-up mechanism and unique approach to action and state space reduction. In our environment, towers level up automatically based on the number of enemies they kill without explicit user input. This adds a layer of complexity for learning optimal solutions. That is, balancing strategic placement for tower level-ups and successful long term  defence.

The tower defence environment has a high dimensional and continuous state, with a large discrete action space. This makes tabular based learning methods infeasible. Conventional methods such as Tabular Q-Learning or SARSA cannot store Q-tables for an infinite space. So, we first chose SARSA with function approximation to handle the high dimensional state space. Also, being an on-policy algorithm, SARSA allows for sufficient coverage of the large action space. Note that linear function approximation struggles with the large state space, so a Q-network was used for the function approximation. Next, we looked at the Advantage Actor Critic Algorithm (A2C) which works with continuous state spaces and discrete action spaces \cite{masteringrl_actor_critic}. Actor-Critic are policy gradient algorithms that optimize the policy directly through learned parameters. This allows us to tune the policy to reduce variance seen in methods such as SARSA (e.g., reducing actor step size).

However, SARSA and A2C are on-policy algorithms, making  convergence to a stable optimal policy difficult. We then explored Deep Q-Learning with Q-Networks to avoid the on-policy nature of SARSA, and utilized a replay buffer to reduce variance. 
\section{Empirical studies}

\subsection{Q-learning}
We implemented Q-learning using a simple 3 layer Q-network as our function approximator and a target network to stabilize updates. The Q-network allowed us to learn in a high dimensional state and action space where tabular Q-learning would be infeasible. The space complexity for tabular learning would be $O(|S| x |A|)$. Specifically, $O(11^{v(n)})$, where $v(n) = 49$ valid squares for $n= 10$ ($10 \times 10$ grid).

The target network was implemented to reduce variance and stabilize learning across episodes. However, this approach did not yield good results. As seen in Figure \ref{fig:q_learning_performance}, no convergence and instability in learning. 

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.7\linewidth]{figures/qlearning_10runs_10000episodes.png}
  \caption{Q-learning performance with different learning rates.}
  \label{fig:q_learning_performance}
\end{figure}

This is due to the deadly triad combination of function approximation, bootstrapping, and off-policy learning. Even when attempting to stabilize bootstrapping updates, there was no consistency in learning. One interesting observation was that higher learning rates yielded more consistent, but low returns. On the other hand, lower learning rates yielded high variance and less consistency. With a lower learning rate, the immediate rewards are slower to propagate through the Q-network, so stochasticity in the environment dominates. This demonstrates Q-learning's sensitivity to hyper-parameters and overall instability.

\subsubsection{SARSA}
After implementing the SARSA algorithm with Q-network function approximation, we evaluated and observed the following results in Figure~\ref{fig:sarsa-performance}. 

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.7\linewidth]{figures/sarsa\_10runs\_10000episodes.png}
  \caption{SARSA performance with different learning rates.}
  \label{fig:sarsa-performance}
\end{figure}

First, the environment has a delayed and sparse reward structure, causing SARSA's on-policy TD(0) update to struggle with the delayed reward signals. Also, with increasingly difficult waves, an action taken in one state may signal a vastly different reward in another state. For example, in an early wave when enemy health is lower, placing a tower in a cell that results in the elimination of the enemy will return a large positive return. However, in later waves when enemy health is high, placing the same tower in the same cell may not yield the same result. This inhibits SARSA learning, since it expects environment dynamics to be stationary to guarantee convergence. Furthermore, SARSA is an on-policy algorithm. It depends directly on actions taken. In early exploration, actions are random which leads to poor estimates, noisy updates, and high variance. 

Due to the on-policy nature of SARSA, we do see more coverage of the action space compared to off-policy methods such as Q-learning. Overall, SARSA performed poorly in learning an optimal policy. Reducing the state dimensionality and ensuring stationary environment dynamics are required for better performance. Other methods should be considered before implementing SARSA in a tower defense environment.

\subsubsection{Advantage Actor-Critic (A2C)}
After running the A2C algorithm with scaled-up rewards, it was observed in Figure~\ref{fig:a2c-performance} to have rough convergence to an optimal policy. 

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.7\linewidth]{figures/a2c\_learn\_1runs\_10000episodes.png}
  \caption{A2C performance with different learning rates.}
  \label{fig:a2c-performance}
\end{figure}

In this reward structure, the estimated maximum value you could reach when interacting with the environment was 3700. In the line chart, in Figure **figure no. here** the algorithm performed well in achieving close to maximum rewards values across runs. However, we do see high variance regardless of the critic's step size. This is due to the on-policy nature of the algorithm, where updates to the actor policy impact the policy for all other actions. Another reason is that A2C uses a SoftMax policy. The action is sampled from a distribution, so there's always a chance that policy will deviate from the optimal action, creating variance in the average evaluated return.

We also noticed the training time for A2C was considerably longer compared to Q-learning and SARSA. 

Overall, the A2C algorithm performed well with this environment, likely due to the critic's ability to make less accurate state-value estimates without drastically impacting the actor policy.

\subsubsection{DQN}
For the DQN experiments, the agent observes the flattened metadata state vector and we apply per-feature min–max normalization before feeding it to the network. The Q-function is represented by a three-layer feed-forward network with two 128-unit hidden layers and ReLU activations, outputting one Q-value per discrete action. To stabilize learning, we follow the standard DQN design. We use an experience replay memory that stores up to 50,000 transitions, mini-batches of size 64 are sampled for updates, and a separate target network is updated every 10 episodes. Exploration is $\epsilon$-greedy with $\epsilon$ decaying exponentially from 1.0 to a minimum of 0.05 over episodes.\\

\begin{figure} [!htbp]
  \centering
  \includegraphics[width=0.5\linewidth]{figures/dqn_3r_2500ep_3learn}
  \caption{DQN with 3 different performance rates}
  \label{fig:dqn_performance}
\end{figure}

Figure X summarizes the learning curves for the three learning rates (0.001, 0.0005, 0.0001). Each configuration was trained for 2,500 episodes and evaluated every 50 episodes for 1 run. All three learning rates show an overall upward trend, indicating that DQN is able to exploit experience replay and the target network to improve performance over time rather than diverging, unlike the earlier Q-learning baseline. The largest learning rate (0.001) achieves the fastest performance gains and reaches the highest final average return, although the curve being more volatile than the others since the update step is larger. The intermediate rate (0.0005) learns more slowly but exhibits a smoother curve with fewer sharp drops, while the smallest rate (0.0001) produces the most stable but also the lowest returns, this suggests underfitting and a very slow propagation of reward information due to the sparse and delayed rewards.\\\\
Overall, these results suggest that DQN is the most effective algorithm that we tested being significantly more effective than our vanilla Q-learning and SARSA with neural function approximation, which showed little convergence and high instability. However, the DQN curves are still quite volatile, reflecting the difficulty of learning in a high-dimensional, stochastic environment with delayed rewards. The sensitivity to the learning rate visible in the figure indicates that larger step sizes can unlock higher returns but at the cost of higher variance, while smaller step sizes are more stable but may fail to fully exploit the environment within a fixed training budget.

\subsubsection{Quantile Regression DQN (QR-DQN)}
To attempt to address the instability observed in standard DQN, we implemented Quantile Regression DQN (QR-DQN), which is a distributional RL algorithm. Rather than learning a single expected Q-value, QR-DQN models the full distribution of returns using a set of quantiles. This may provide better information about uncertainty in value estimates, which can improve learning stability, especially in a more stochastic environment like this. This algorithm was implemented using the SB3 library with 50 quantiles. 

Our experiment ran for 100,000 timesteps, evaluating every 5,000 and averaging over 3 runs. Three learning rates were tested: 0.001, 0.0005, and 0.0001. The results are shown in Figure~\ref{fig:qrdqn_performance}.

\begin{figure}[!htbp]
  \centering
  \includegraphics[width=0.6\linewidth]{figures/qrdqn_learn.png}
  \caption{QR-DQN performance with different learning rates.}
  \label{fig:qrdqn_performance}
\end{figure}

The highest learning rate (0.001) showed steady improvement throughout training, reaching an average evaluated return of around 7,000 by the end. Despite an initial dip around 35,000 timesteps, the algorithm recovered and showed consistent upward progress. The middle learning rate (0.0005) exhibited faster early learning, peaking at approximately 6,500 around 55,000 timesteps. However, performance degraded in later timesteps, suggesting that there may have been overfitting or oscillation happening. The lowest learning rate (0.0001) failed to learn effectively, remaining near zero with one spike around 35,000 timesteps, suggesting that there was insufficient adaptation.

QR-DQN outperformed both Q-learning and SARSA, but it did not surpass standard DQN, which achieved returns in a much higher range. The more complex modelling of returns may have hindered the learning efficiency within the training budget. As well, the approach of this algorithm added computational overhead and training time without much benefit. The simpler DQN with experience replay appears better suited for this environment.



\section{Conclusion}
In conclusion, we explored various reinforcement learning (RL) methods to learn an optimal policy for tower defense. This included Q-learning, SARSA, Advantage Actor-Critic (A2C), Proximal Policy Optimization (PPO), Deep Q-Networks (DQN), and Quantile Regression DQN.

We observed high variability and poor performance for the Q-learning and SARSA algorithms. This is due to the difficulty of bootstrapping in high dimensional state spaces with sparse and delayed rewards. The deadly triad of function approximation, bootstrapping, and off-policy learning proved particularly problematic for Q-learning, while SARSA struggled with non-stationary environment dynamics as wave difficulty increased.

Policy gradient-based algorithms such as A2C and PPO yielded better results. This is due to their ability to learn the policy directly, instead of relying on bootstrapped Q-values. A2C achieved returns close to the theoretical maximum of 3,700, demonstrating that actor-critic methods are well-suited for this environment. PPO showed sensitivity to the clip range hyperparameter, with smaller clip values learning faster but less stably.

DQN emerged as our best performing algorithm, achieving returns in the 10,000-40,000 range. The combination of experience replay and target networks effectively addressed the instability seen in vanilla Q-learning. Interestingly, QR-DQN did not improve upon standard DQN despite its distributional approach, suggesting that modelling return uncertainty provides limited benefit in this environment and adds unnecessary computational overhead.

A key insight from this project is the importance of matching algorithm complexity to the problem. Simpler methods with appropriate stabilization techniques (e.g., DQN with replay buffers) outperformed more sophisticated distributional approaches. Additionally, the automatic tower evolution mechanic created a challenging credit assignment problem, where early placement decisions have delayed effects on tower strength.

In future work, more analysis and design of a strong feature extractor should be considered. This would help reduce the high dimensional state complexity and improve algorithm performance. Reward shaping to provide denser feedback signals could also accelerate learning. Next, other algorithms should be considered. To address overestimation, an algorithm such as Double Deep Q-Networks may be considered. For more parallelized training, Asynchronous Advantage Actor-Critic can be considered. In terms of training, curriculum learning may help agents learn fundamental strategies before facing the full complexity of the environment (i.e.\ starting with fewer waves and gradually increasing difficulty).

Overall, we gained valuable insights into the application of RL in a tower defence environment. This environment proved to be a challenging testbed for RL due to a combination of strategic planning, resource management, and stochasticity. In particular, we learned about the tradeoffs between an algorithm's complexity, stability, and efficiency.

\section{Citations, figures, tables, references}
\label{others}


These instructions apply to everyone.


\subsection{Citations within the text}


The \verb+natbib+ package will be loaded for you by default.  Citations may be
author/year or numeric, as long as you maintain internal consistency.  As to the
format of the references themselves, any style is acceptable as long as it is
used consistently.


The documentation for \verb+natbib+ may be found at
\begin{center}
  \url{http://mirrors.ctan.org/macros/latex/contrib/natbib/natnotes.pdf}
\end{center}
Of note is the command \verb+\citet+, which produces citations appropriate for
use in inline text.  For example,
\begin{verbatim}
   \citet{hasselmo} investigated\dots
\end{verbatim}
produces
\begin{quote}
  Hasselmo, et al.\ (1995) investigated\dots
\end{quote}


If you wish to load the \verb+natbib+ package with options, you may add the
following before loading the \verb+neurips_2023+ package:
\begin{verbatim}
   \PassOptionsToPackage{options}{natbib}
\end{verbatim}


If \verb+natbib+ clashes with another package you load, you can add the optional
argument \verb+nonatbib+ when loading the style file:
\begin{verbatim}
   \usepackage[nonatbib]{neurips_2023}
\end{verbatim}


As submission is double blind, refer to your own published work in the third
person. That is, use ``In the previous work of Jones et al.\ [4],'' not ``In our
previous work [4].'' If you cite your other papers that are not widely available
(e.g., a journal paper under review), use anonymous author names in the
citation, e.g., an author of the form ``A.\ Anonymous'' and include a copy of the anonymized paper in the supplementary material.


\subsection{Footnotes}


Footnotes should be used sparingly.  If you do require a footnote, indicate
footnotes with a number\footnote{Sample of the first footnote.} in the
text. Place the footnotes at the bottom of the page on which they appear.
Precede the footnote with a horizontal rule of 2~inches (12~picas).


Note that footnotes are properly typeset \emph{after} punctuation
marks.\footnote{As in this example.}


\subsection{Figures}


\begin{figure}
  \centering
  \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
  \caption{Sample figure caption.}
\end{figure}


All artwork must be neat, clean, and legible. Lines should be dark enough for
purposes of reproduction. The figure number and caption always appear after the
figure. Place one line space before the figure caption and one line space after
the figure. The figure caption should be lower case (except for first word and
proper nouns); figures are numbered consecutively.


You may use color figures.  However, it is best for the figure captions and the
paper body to be legible if the paper is printed in either black/white or in
color.


\subsection{Tables}


% All tables must be centered, neat, clean and legible.  The table number and
% title always appear before the table.  See Table~\ref{sample-table}.


Place one line space before the table title, one line space after the
table title, and one line space after the table. The table title must
be lower case (except for first word and proper nouns); tables are
numbered consecutively.


% Note that publication-quality tables \emph{do not contain vertical rules.} We
% strongly suggest the use of the \verb+booktabs+ package, which allows for
% typesetting high-quality, professional tables:
% \begin{center}
%   \url{https://www.ctan.org/pkg/booktabs}
% \end{center}
% This package was used to typeset Table~\ref{sample-table}.


\begin{table}
  \caption{Sample table title}
  \label{sample-table}
  \centering
  \begin{tabular}{lll}
    \toprule
    \multicolumn{2}{c}{Part}                   \\
    \cmidrule(r){1-2}
    Name     & Description     & Size ($\mu$m) \\
    \midrule
    Dendrite & Input terminal  & $\sim$100     \\
    Axon     & Output terminal & $\sim$10      \\
    Soma     & Cell body       & up to $10^6$  \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Math}
Note that display math in bare TeX commands will not create correct line numbers for submission. Please use LaTeX (or AMSTeX) commands for unnumbered display math. (You really shouldn't be using \$\$ anyway; see \url{https://tex.stackexchange.com/questions/503/why-is-preferable-to} and \url{https://tex.stackexchange.com/questions/40492/what-are-the-differences-between-align-equation-and-displaymath} for more information.)

\subsection{Final instructions}

Do not change any aspects of the formatting parameters in the style files.  In
particular, do not modify the width or length of the rectangle the text should
fit into, and do not change font sizes (except perhaps in the
\textbf{References} section; see below). Please note that pages should be
numbered.


\section{Preparing PDF files}


Please prepare submission files with paper size ``US Letter,'' and not, for
example, ``A4.''


Fonts were the main cause of problems in the past years. Your PDF file must only
contain Type 1 or Embedded TrueType fonts. Here are a few instructions to
achieve this.


\begin{itemize}


\item You should directly generate PDF files using \verb+pdflatex+.


\item You can check which fonts a PDF files uses.  In Acrobat Reader, select the
  menu Files$>$Document Properties$>$Fonts and select Show All Fonts. You can
  also use the program \verb+pdffonts+ which comes with \verb+xpdf+ and is
  available out-of-the-box on most Linux machines.


\item \verb+xfig+ "patterned" shapes are implemented with bitmap fonts.  Use
  "solid" shapes instead.


\item The \verb+\bbold+ package almost always uses bitmap fonts.  You should use
  the equivalent AMS Fonts:
\begin{verbatim}
   \usepackage{amsfonts}
\end{verbatim}
followed by, e.g., \verb+\mathbb{R}+, \verb+\mathbb{N}+, or \verb+\mathbb{C}+
for $\mathbb{R}$, $\mathbb{N}$ or $\mathbb{C}$.  You can also use the following
workaround for reals, natural and complex:
\begin{verbatim}
   \newcommand{\RR}{I\!\!R} %real numbers
   \newcommand{\Nat}{I\!\!N} %natural numbers
   \newcommand{\CC}{I\!\!\!\!C} %complex numbers
\end{verbatim}
Note that \verb+amsfonts+ is automatically loaded by the \verb+amssymb+ package.


\end{itemize}


If your file contains type 3 fonts or non embedded TrueType fonts, we will ask
you to fix it.


\subsection{Margins in \LaTeX{}}


Most of the margin problems come from figures positioned by hand using
\verb+\special+ or other commands. We suggest using the command
\verb+\includegraphics+ from the \verb+graphicx+ package. Always specify the
figure width as a multiple of the line width as in the example below:
\begin{verbatim}
   \usepackage[pdftex]{graphicx} ...
   \includegraphics[width=0.8\linewidth]{myfile.pdf}
\end{verbatim}
See Section 4.4 in the graphics bundle documentation
(\url{http://mirrors.ctan.org/macros/latex/required/graphics/grfguide.pdf})


A number of width problems arise when \LaTeX{} cannot properly hyphenate a
line. Please give LaTeX hyphenation hints using the \verb+\-+ command when
necessary.


\begin{ack}
Use unnumbered first level headings for the acknowledgments. All acknowledgments
go at the end of the paper before the list of references. Moreover, you are required to declare
funding (financial activities supporting the submitted work) and competing interests (related financial activities outside the submitted work).
More information about this disclosure can be found at: \url{https://neurips.cc/Conferences/2023/PaperInformation/FundingDisclosure}.


Do {\bf not} include this section in the anonymized submission, only in the final paper. You can use the \texttt{ack} environment provided in the style file to autmoatically hide this section in the anonymized submission.
\end{ack}



\section{Supplementary Material}

Authors may wish to optionally include extra information (complete proofs, additional experiments and plots) in the appendix. All such materials should be part of the supplemental material (submitted separately) and should NOT be included in the main submission.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliographystyle{unsrtnat}  % or plainnat
\bibliography{references}
\end{document}
