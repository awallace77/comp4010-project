\documentclass[12pt]{article}

\usepackage{fullpage}
\usepackage{fancybox}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{enumerate}
\usepackage{parskip}
\usepackage{hyperref} 

%%%%%%%%%%%%%% Capsule %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\capsule}[2]{\vspace{0.5em}
  \shadowbox{%
    \begin{minipage}{.90\linewidth}%
      \textbf{#1:}~#2%
    \end{minipage}}
  \vspace{0.5em} }
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcounter{ques}
\newenvironment{question}{\stepcounter{ques}{\noindent\bf Question \arabic{ques}:}}{\vspace{5mm}}

\begin{document} 

\begin{center} \Large\bf
RL for Tower Defense with Evolutionary Towers\\
\end{center} 

\begin{center}
Group 12 \\
Andrew Wallace - 101210291 - andrewwallace3@cmail.carleton.ca\\
Mohammad Rehman - 101220514 - mohammadrehman@cmail.carleton.ca \\
Manal Hassan - 101263813 \\
Derrick Zhang
\end{center}
\newpage 

\section*{Problem Statement}
We aim to develop and train a reinforcement learning agent to play an evolving tower defense game where strategic resource allocation and adaptive tower placement are critical for success. Unlike traditional tower defense games with manual upgrades, our environment features towers that automatically evolve and increase in strength based on the number of enemies they eliminate, creating a feedback loop between decisions and long-term strategy when it comes to positioning.\par
The agent must learn to place two types of towers: single target and area-of-effect. The environment will be grid-based with limited budget constraints per wave. As enemy waves progress with increasing health, the agent must balance immediate defensive needs with long-term tower development, deciding not only where to place towers but also which towers to prioritize for enemy elimination to maximize evolutionary potential.\par

This problem presents a few challenges: immediate survival against long-term tower development, spatial reasoning for optimal tower placement on a constrained grid, resource allocation under budget constraints, and adaptation to increasingly difficult enemy waves. \par

The evolving tower mechanism introduces a novel aspect where early decisions significantly impact later capabilities, making this an interesting environment for a reinforcement learning algorithms that must learn both immediate responses and long-term strategic planning. \par

\section*{Feasibility}

Reinforcement learning (RL) is well-suited for this problem because the environment is stochastic, sequential, and dynamic, with an agent's actions influencing future states. \par 

As the enemy waves increase in difficulty, the agent must adapt in terms of where to place towers, what type of towers to place, and their strategy to maximize their chances of survival. The sequential relationship of agent action and future success is a delayed reward optimization problem that RL methods, such as Q-learning, specialize in, making RL a good fit for this problem. [2] \par

Furthermore, the presence of two tower types, budget constraints, enemy health, and a grid-based environment creates a large and dynamic state space, making traditional machine learning techniques infeasible. This further supports the case for reinforcement learning. [3] \par

On the other hand, RL alone may be infeasible. Similar work in a grid-based tower defense game has shown that, ``A fully-fledged RL agent, trained to select one of the units and place it wherever desired, is hardly feasible in this case'' [4]. As such, techniques beyond reinforcement learning alone may be considered in this project. \par

Lastly, the evolving tower mechanism further complicates the balance of immediate survival and long-term tower development, which is a direct RL use case. \par

Overall, RL is well-suited for this problem due to its ability to handle delayed rewards, dynamic environments, and complex sequential decision-making, all of which are essential in a successful long-term tower defense strategy.


\section*{Milestones}
\textbf{Initial Environment Setup (September 29 -- October 13)}
\begin{itemize}
    \item Implement basic gym environment
    \item Submit bi-weekly progress report due (Oct 15)
\end{itemize}

\textbf{Final Environment Implementation (October 14 -- October 27)}
\begin{itemize}
    \item Complete custom gym environment
    \item Test environment with random agent
    \item Submit environment demo video (10 minutes)
\end{itemize}

\textbf{Training and Initial Results (October 28 -- November 10)}
\begin{itemize}
    \item Research relevant RL algorithms for our use case
    \item Start implementing RL algorithms
    \item Submit second bi-weekly progress report (Oct 30)
\end{itemize}

\textbf{Final Algorithm Implementation (November 11 -- November 24)}
\begin{itemize}
    \item Have a working RL algorithms
    \item Gather initial training results
    \item Submit third bi-weekly progress report (Nov 15)
\end{itemize}

\textbf{Result Demo (November 25 -- December 1)}
\begin{itemize}
    \item Gather and complete the experimental results
    \item Different algorithm comparison analysis
    \item Fourth bi-weekly progress report (Nov 30)
    \item Submit demo video (Dec 1)
\end{itemize}

\textbf{Final Report Writing (December 2 -- December 7)}
\begin{itemize}
    \item Every group member to write a section of the report
    \item Gather and review the final report for final draft
    \item Submit final report (Dec 8)
\end{itemize}

\vspace{0.5em}
\newpage 

\section*{References}

[1] Sindhu Padakandla. 2021. A Survey of Reinforcement Learning Algorithms for Dynamically Varying Environments. ACM Comput. Surv. 54, 6, Article 127 (July 2021), 25 pages.
https://doi.org/10.1145/3459991 \par

[2] J. Clifton, E. Laber. (2020). Q-Learning: Theory and Applications. [ONLINE]. Available: https://www.annualreviews.org/content/journals/10.1146/annurev-statistics-031219-041220 \par

[3] Dias, A., Foleiss, J., Lopes, R.P. (2022). Reinforcement Learning in Tower Defense. In: Barbedo, I., Barroso, B., Legerén, B., Roque, L., Sousa, J.P. (eds) Videogame Sciences and Arts. VJ 2020. Communications in Computer and Information Science, vol 1531. Springer, Cham. https://doi.org/10.1007/978-3-030-95305-8\_10 \par 

[4] J. Bergdahl, A. Sestini and L. Gisslén, "Reinforcement Learning for High-Level Strategic Control in Tower Defense Games," 2024 IEEE Conference on Games (CoG), Milan, Italy, 2024, pp. 1-8, doi: 10.1109/CoG60054.2024.10645621 \par

[5] Gym library. https://gym.openai.com/. Accessed September 20 2025

\end{document} 
